# MNIST-Ensemble-Learning-Project

This repository contains a full implementation of **ensemble learning techniques** using the **MNIST handwritten digits dataset**.  
The project demonstrates how combining multiple modelsâ€”through voting and stackingâ€”can improve performance compared to individual classifiers.

This notebook was created as part of a Machine Learning assignment and showcases practical ML engineering skills.

## ğŸš€ Project Summary

We train three base classifiers:

- **Random Forest Classifier**  
- **Extra Trees Classifier**  
- **SVM (RBF Kernel)** (with feature scaling)

Then we build multiple ensemble models:

- **Hard Voting Ensemble**  
- **Soft Voting Ensemble**  
- **Manual Stacking / Blender Model**  
- **`StackingClassifier` from Scikit-Learn**

The goal is to compare validation and test accuracy across these models and determine whether ensembling provides measurable performance gains.

## ğŸ§  Key Concepts Demonstrated

âœ” Bagging-based models (Random Forest, Extra Trees)  
âœ” SVM classification with scaling  
âœ” Hard vs. soft voting  
âœ” Meta-learning with stacking  
âœ” Model comparison and evaluation  
âœ” Train/validation/test splitting  
âœ” Working with scikit-learn pipelines  

This project is ideal for anyone learning ensemble methods or building an ML portfolio.

## ğŸ“Š Dataset

The MNIST dataset (from OpenML) contains:

- **70,000** grayscale images of handwritten digits
- **28 Ã— 28** pixel resolution
- Classes **0â€“9**

The dataset is split as:

- **50,000** for training  
- **10,000** for validation  
- **10,000** for testing  

## ğŸ“‚ Repository Structure

```
mnist-ensemble-learning/
â”‚
â”œâ”€â”€ ensemble_learning_mnist.ipynb   # Main notebook with all training code
â””â”€â”€ README.md                       # Documentation
```

## â–¶ï¸ How to Run

### Option 1: Google Colab
1. Upload the notebook  
2. Run all cells  
3. No installs required  

### Option 2: Local Environment

Install dependencies:

```bash
pip install numpy scikit-learn matplotlib
```

Start Jupyter:

```bash
jupyter notebook ensemble_learning_mnist.ipynb
```

## ğŸ“ˆ Expected Performance

| Model | Approx. Validation Accuracy |
|-------|-----------------------------|
| Random Forest | 96â€“97% |
| Extra Trees | ~97% |
| SVM (RBF) | ~98% |
| **Soft Voting Ensemble** | ~98â€“98.5% |
| **Manual Stacking** | ~98â€“99% |
| **StackingClassifier** | Often best (up to 99%) |

Your results may vary depending on system performance.

## ğŸ§© Skills Shown in This Project

This notebook demonstrates:

- Hands-on machine learning model training  
- Ensemble learning (bagging, voting, stacking)  
- Advanced scikit-learn techniques  
- Data preprocessing  
- Evaluation & comparison of multiple ML models  
- Reproducible ML workflow design  

## âœ¨ Author

**Chastity Lewis**  
Graduate Student â€“ Computer Science (AI & Machine Learning)  
Machine Learning Engineer (in progress)  
Creator of beauty-tech brand **Chasslayy Luxe**
